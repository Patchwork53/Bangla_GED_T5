{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Training Notebook\nBase Model: https://huggingface.co/csebuetnlp/banglat5_small <br>\nTrained Model: https://www.kaggle.com/datasets/sameen53/schadenfreude-bhashabhrom <br>\n(Trained model is the output of this notebook Version 11)\nEffective Batch Size : 128 <br>\nEpoch : 120 <br>\nDataset : DataSetFold1.csv <br>\n\nInitially we were provided 9.3k training samples and asked to infer on 5k training samples. To maximize training data, we ran tests with different splits and determined 120 epochs to be a good stopping point before overfitting. Then we used the entirety of DataSetFold1.csv for training. We did not use DataSetFold2.csv for training the model. <br>\n\n### Version History\nVersion 11 Output contains the final model we used in our submissions. Logs of Version 1-10 show our experiments to find optimal hyperparameters <br>\n\n### Output\nThis notebook outputs:\n* The trained model in the folder kaggle/working/banglat5\n* The last 2 checkpoints from training\n* (commented out) Prediction on train set for testing purposes  kaggle/working/banglat5/training_predictions.txt\n* Prediction on test set kaggle/working/banglat5/generated_predictions.txt\n\nNB: generated_predictions.txt can not be submitted as is. It must go through the steps as described in the inference notebook.\n\n\n### Credit\nCode was adapted from https://github.com/csebuetnlp/BanglaNLG/blob/main/seq2seq/run_seq2seq.py <br>\nDataset provided by the Hosts of EEEDAY 2023 Datathon\n\n\n","metadata":{}},{"cell_type":"code","source":"!pip install -q sentencepiece\n!pip install -q protobuf\n!pip install -q datasets\n!pip install -q seqeval\n!pip install -q sacrebleu\n!pip install -q git+https://github.com/csebuetnlp/normalizer\n!pip install -q git+https://github.com/otuncelli/turkish-stemmer-python\n!pip install -q git+https://github.com/abhik1505040/bengali-stemmer\n!pip install -q git+https://github.com/abhik1505040/indic_nlp_library\n!pip install -q absl-py\n!pip install -q nltk\n!pip install -q numpy\n!pip install -q six\n!pip install -q pythainlp\n!pip install -q transformers\n!pip install -q rouge_score\n!pip install -q pyonmttok\n!pip install -q jieba\n!pip install -q levenshtein\n!pip install -q fugashi[unidic]\n!pip install -q huggingface_hub\n!pip install -q git+https://github.com/csebuetnlp/xl-sum.git#egg=rouge_score&subdirectory=multilingual_rouge_scoring","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-03-10T02:53:36.761365Z","iopub.execute_input":"2023-03-10T02:53:36.762271Z","iopub.status.idle":"2023-03-10T02:57:27.069412Z","shell.execute_reply.started":"2023-03-10T02:53:36.762153Z","shell.execute_reply":"2023-03-10T02:57:27.068236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"code","source":"!mkdir json_data","metadata":{"execution":{"iopub.status.busy":"2023-03-10T06:07:27.181551Z","iopub.execute_input":"2023-03-10T06:07:27.182472Z","iopub.status.idle":"2023-03-10T06:07:28.302538Z","shell.execute_reply.started":"2023-03-10T06:07:27.182379Z","shell.execute_reply":"2023-03-10T06:07:28.301349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport codecs\nimport sys\n\npath = \"/kaggle/input/eeeday-fold1/DataSetFold1.csv\"\ndf = pd.read_csv(path, encoding = 'utf8')\n\ncount = 0\nwith codecs.open(\"json_data/train.json\", encoding = 'utf8', mode='w') as f:\n    for idx, row in df.iterrows():\n        sentence = row[\"sentence\"]\n        gt = row[\"gt\"]\n       \n        sentence = sentence.replace(\"\\\"\", \"\\\\\\\"\")\n        # gt = row[\"gt\"]\n        gt = gt.replace(\"\\\"\", \"\\\\\\\"\")\n        f.write(\"{\\\"source\\\": \\\"\" + sentence + \"\\\", \\\"target\\\": \\\"\" + gt + \"\\\"}\\n\")\n\nprint(count)\n\n\ndf = pd.read_csv(\"/kaggle/input/eeeday-fold1/test.csv\", encoding = 'utf8')\n\nwith codecs.open(\"json_data/test.json\", encoding = 'utf8', mode='w') as f:\n    for idx, row in df.iterrows():\n        sentence = row[\"text\"]\n        sentence = sentence.replace(\"\\\"\", \"\\\\\\\"\")\n        gt = \"\"\n        # gt = row[\"gt\"]\n        # gt = gt.replace(\"\\\"\", \"\\\\\\\"\")\n        f.write(\"{\\\"source\\\": \\\"\" + sentence + \"\\\", \\\"target\\\": \\\"\" + gt + \"\\\"}\\n\")","metadata":{"execution":{"iopub.status.busy":"2023-03-10T06:07:28.304200Z","iopub.execute_input":"2023-03-10T06:07:28.304536Z","iopub.status.idle":"2023-03-10T06:07:29.363824Z","shell.execute_reply.started":"2023-03-10T06:07:28.304508Z","shell.execute_reply":"2023-03-10T06:07:29.362956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!tail -n 128 /kaggle/working/json_data/train.json > /kaggle/working/json_data/validation.json","metadata":{"execution":{"iopub.status.busy":"2023-03-10T02:57:28.873008Z","iopub.execute_input":"2023-03-10T02:57:28.874474Z","iopub.status.idle":"2023-03-10T02:57:29.819659Z","shell.execute_reply.started":"2023-03-10T02:57:28.874434Z","shell.execute_reply":"2023-03-10T02:57:29.818365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"import logging\nimport os\nimport sys\nimport glob\nimport json\nfrom dataclasses import dataclass, field\nfrom typing import Optional\nfrom Levenshtein import distance\n\nimport datasets\nimport nltk\nimport numpy as np\nfrom sacrebleu import corpus_bleu\nfrom rouge_score import rouge_scorer, scoring\nfrom datasets import load_dataset, load_metric\nfrom datasets.io.json import JsonDatasetReader\nfrom datasets.io.csv import CsvDatasetReader\n\nimport transformers\nfrom transformers import (\n    AutoConfig,\n    AutoModelForSeq2SeqLM,\n    AutoTokenizer,\n    DataCollatorForSeq2Seq,\n    HfArgumentParser,\n    M2M100Tokenizer,\n    MBart50Tokenizer,\n    MBart50TokenizerFast,\n    MBartTokenizer,\n    MBartTokenizerFast,\n    MBartForConditionalGeneration,\n    AlbertTokenizer,\n    AlbertTokenizerFast,\n    Seq2SeqTrainer,\n    Seq2SeqTrainingArguments,\n    default_data_collator,\n    set_seed,\n)\nfrom transformers.trainer_utils import get_last_checkpoint\nfrom transformers.utils import check_min_version\nfrom transformers.utils.versions import require_version\nfrom normalizer import normalize\n\nEXT2CONFIG = {\n    \"csv\" : (CsvDatasetReader, {}),\n    \"tsv\" : (CsvDatasetReader, {\"sep\": \"\\t\"}),\n    \"jsonl\": (JsonDatasetReader, {}),\n    \"json\": (JsonDatasetReader, {})\n}\n\n\nlogger = logging.getLogger(__name__)\n\n\n@dataclass\nclass ModelArguments:\n    \n    model_name_or_path: str = field(\n        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n    )\n    cache_dir: Optional[str] = field(\n        default=None,\n        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n    )\n\n\n@dataclass\nclass DataTrainingArguments:\n    dataset_dir: Optional[str] = field(\n        default=None, metadata={\n            \"help\": \"Path to the directory containing the data files. (.csv / .tsv / .jsonl)\"\n            \"File datatypes will be identified with their prefix names as follows: \"\n            \"`train`- Training file(s) e.g. `train.csv`/ `train_part1.csv` etc. \"\n            \"`validation`- Evaluation file(s) e.g. `validation.csv`/ `validation_part1.csv` etc. \"\n            \"`test`- Test file(s) e.g. `test.csv`/ `test_part1.csv` etc. \"\n            \"All files for must have the same extension.\"\n        }\n    )\n    \n    dataset_name: Optional[str] = field(\n        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n    )\n    dataset_config_name: Optional[str] = field(\n        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n    )\n    \n    overwrite_cache: bool = field(\n        default=False, metadata={\"help\": \"Overwrite the cached preprocessed datasets or not.\"}\n    )\n    max_train_samples: Optional[int] = field(\n        default=None,\n        metadata={\n            \"help\": \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n            \"value if set.\"\n        },\n    )\n    max_eval_samples: Optional[int] = field(\n        default=None,\n        metadata={\n            \"help\": \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n            \"value if set.\"\n        },\n    )\n    max_predict_samples: Optional[int] = field(\n        default=None,\n        metadata={\n            \"help\": \"For debugging purposes or quicker training, truncate the number of prediction examples to this \"\n            \"value if set.\"\n        },\n    )\n    train_file: Optional[str] = field(\n        default=None, metadata={\"help\": \"A csv / tsv / jsonl file containing the training data.\"}\n    )\n    validation_file: Optional[str] = field(\n        default=None, metadata={\"help\": \"A csv / tsv / jsonl file containing the validation data.\"}\n    )\n    test_file: Optional[str] = field(default=None, metadata={\"help\": \"A csv / tsv / jsonl file containing the test data.\"})\n    do_normalize: Optional[bool] = field(default=False, metadata={\"help\": \"Normalize text before feeding to the model.\"})\n    unicode_norm: Optional[str] = field(default=\"NFKC\", metadata={\"help\": \"Type of unicode normalization\"})\n    remove_punct: Optional[bool] = field(\n        default=False, metadata={\n            \"help\": \"Remove punctuation during normalization. To replace with custom token / selective replacement you should \"\n            \"use this repo (https://github.com/abhik1505040/normalizer) before feeding the data to the script.\"\n    })\n    remove_emoji: Optional[bool] = field(\n        default=False, metadata={\n            \"help\": \"Remove emojis during normalization. To replace with custom token / selective replacement you should \"\n            \"use this repo (https://github.com/abhik1505040/normalizer) before feeding the data to the script.\"\n    })\n    remove_urls: Optional[bool] = field(\n        default=False, metadata={\n            \"help\": \"Remove urls during normalization. To replace with custom token / selective replacement you should \"\n            \"use this repo (https://github.com/abhik1505040/normalizer) before feeding the data to the script.\"\n    })\n    source_key: Optional[str] = field(\n        default=\"source\", metadata={\"help\": \"Key / column name in the input file corresponding to the source data\"}\n    )\n    target_key: Optional[str] = field(\n        default=\"target\", metadata={\"help\": \"Key / column name in the input file corresponding to the target data\"}\n    )\n\n    source_lang: Optional[str] = field(default=None, metadata={\"help\": \"Source language id.\"})\n    target_lang: Optional[str] = field(default=None, metadata={\"help\": \"Target language id.\"})\n\n    preprocessing_num_workers: Optional[int] = field(\n        default=None,\n        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n    )\n    max_source_length: Optional[int] = field(\n        default=1024,\n        metadata={\n            \"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\n            \"than this will be truncated, sequences shorter will be padded.\"\n        },\n    )\n    max_target_length: Optional[int] = field(\n        default=128,\n        metadata={\n            \"help\": \"The maximum total sequence length for target text after tokenization. Sequences longer \"\n            \"than this will be truncated, sequences shorter will be padded.\"\n        },\n    )\n    val_max_target_length: Optional[int] = field(\n        default=128,\n        metadata={\n            \"help\": \"The maximum total sequence length for validation target text after tokenization. Sequences longer \"\n            \"than this will be truncated, sequences shorter will be padded. Will default to `max_target_length`.\"\n            \"This argument is also used to override the ``max_length`` param of ``model.generate``, which is used \"\n            \"during ``evaluate`` and ``predict``.\"\n        },\n    )\n    \n    num_beams: Optional[int] = field(\n        default=5,\n        metadata={\n            \"help\": \"Number of beams to use for evaluation. This argument will be passed to ``model.generate``, \"\n            \"which is used during ``evaluate`` and ``predict``.\"\n        },\n    )\n    \n    source_prefix: Optional[str] = field(\n        default=None, metadata={\"help\": \"A prefix to add before every source text.\"}\n    )\n    evaluation_metric: Optional[str] = field(\n        default=\"rouge\",\n        metadata={\n            \"help\": \"Evaluation metric\",\n            \"choices\": [\"rouge\", \"sacrebleu\"]\n        }    \n    )\n    rouge_lang: Optional[str] = field(\n        default=None,\n        metadata={\n            \"help\": \"Target language for rouge\",\n        }    \n    )\n\n    def __post_init__(self):\n        if self.train_file is not None and self.validation_file is not None:\n            train_extension = self.train_file.split(\".\")[-1]\n            assert train_extension in [\"csv\", \"jsonl\", \"tsv\", \"json\"], \"`train_file` should be a csv / tsv / jsonl file.\"\n            validation_extension = self.validation_file.split(\".\")[-1]\n            assert (\n                validation_extension == train_extension\n            ), \"`validation_file` should have the same extension csv / tsv / jsonl as `train_file`.\"\n\n\ndef main( model_args, data_args, training_args):\n   \n \n    # Setup logging\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\n        handlers=[logging.StreamHandler(sys.stdout)],\n    )\n\n    log_level = training_args.get_process_log_level()\n    logger.setLevel(log_level)\n    datasets.utils.logging.set_verbosity(log_level)\n    transformers.utils.logging.set_verbosity(log_level)\n    transformers.utils.logging.enable_default_handler()\n    transformers.utils.logging.enable_explicit_format()\n\n    # Log on each process the small summary:\n    logger.warning(\n        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n        + f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n    )\n    logger.info(f\"Training/evaluation parameters {training_args}\")\n\n    set_seed(training_args.seed)\n    has_ext = lambda path: len(os.path.basename(path).split(\".\")) > 1\n    get_ext = lambda path: os.path.basename(path).split(\".\")[-1]\n\n\n    if data_args.dataset_name is not None:\n        # Downloading and loading a dataset from the hub.\n        raw_datasets = load_dataset(\n            data_args.dataset_name, data_args.dataset_config_name, cache_dir=model_args.cache_dir\n        )\n\n    elif data_args.dataset_dir is not None:\n        data_files = {}\n        all_files = glob.glob(\n            os.path.join(\n                data_args.dataset_dir,\n                \"*\"\n            )\n        )\n        all_exts = [get_ext(k) for k in all_files if has_ext(k)]\n        if not all_exts:\n            raise ValueError(\"The `dataset_dir` doesnt have any valid file.\")\n            \n        selected_ext = max(set(all_exts), key=all_exts.count)\n        for search_prefix in [\"train\", \"validation\", \"test\"]:\n            found_files = glob.glob(\n                os.path.join(\n                    data_args.dataset_dir,\n                    search_prefix + \"*\" + selected_ext\n                )\n            )\n            if not found_files:\n                continue\n\n            data_files[search_prefix] = found_files\n\n        dataset_configs = EXT2CONFIG[selected_ext]\n        raw_datasets = dataset_configs[0](\n            data_files, \n            **dataset_configs[1]\n        ).read()\n        \n    else:\n        data_files = {\n            \"train\": data_args.train_file, \n            \"validation\": data_args.validation_file,\n            \"test\": data_args.test_file\n        }\n\n        data_files = {k: v for k, v in data_files.items() if v is not None}\n        \n        if not data_files:\n            raise ValueError(\"No valid input file found.\")\n\n        selected_ext = get_ext(list(data_files.values())[0])\n\n        dataset_configs = EXT2CONFIG[selected_ext]\n        raw_datasets = dataset_configs[0](\n            data_files, \n            **dataset_configs[1]\n        ).read()\n\n    config = AutoConfig.from_pretrained(\n        model_args.model_name_or_path,\n        cache_dir=model_args.cache_dir,     \n    )\n\n    tokenizer = AutoTokenizer.from_pretrained(\n        model_args.model_name_or_path,\n        cache_dir=model_args.cache_dir,\n        use_fast=False\n    )\n    \n    model = AutoModelForSeq2SeqLM.from_pretrained(\n        model_args.model_name_or_path,\n        config=config,\n        cache_dir=model_args.cache_dir,\n    )\n\n    # whether this model is indicbart or its derivative\n    is_indicbart = False\n    # whether this model is the unified_script variant of IndicBART\n    is_unified = False\n\n    if isinstance(model, MBartForConditionalGeneration) and isinstance(tokenizer, AlbertTokenizer):\n        is_indicbart = True\n        from indicnlp.transliterate.unicode_transliterate import UnicodeIndicTransliterator\n        import unicodedata\n        from collections import Counter\n\n        def get_token_family(token):\n            names = Counter([unicodedata.name(c, 'UNKNOWN').split()[0] for c in token])\n            return names.most_common(1)[0][0]\n\n        family2count = Counter([get_token_family(t) for t in tokenizer.get_vocab()])\n        # enumerating most probable families to allow for moderate vocab change\n        ss_requred_unicode_families = [\"BENGALI\", \"TAMIL\", \"MALAYALAM\", \"TELUGU\", \"GURMUKHI\", \"KANNADA\", \"GUJARATI\", \"ORIYA\"]\n        required_unicode_tokens = sum(family2count.get(k, 0) for k in ss_requred_unicode_families)\n\n        if family2count.get(\"DEVANAGARI\", 0) > required_unicode_tokens:\n            is_unified = True\n        \n        logger.info(f\"IndicBART variant: {'US' if is_unified else 'SS'}\")\n\n        code2script = {f\"<2{k}>\": k for k in ['as', 'bn', 'gu', 'hi', 'kn', 'ml', 'mr', 'or', 'pa', 'ta', 'te']}\n        bos_id = tokenizer._convert_token_to_id_with_added_voc(\"<s>\")\n        eos_id = tokenizer._convert_token_to_id_with_added_voc(\"</s>\")\n        pad_id = tokenizer._convert_token_to_id_with_added_voc(\"<pad>\")\n        \n        tokenizer.do_lower_case = False\n        tokenizer.keep_accents = True\n        \n        model.config.pad_token_id = pad_id\n        model.config.bos_token_id = bos_id \n        model.config.eos_token_id = eos_id\n        \n    model.resize_token_embeddings(len(tokenizer))\n\n    if data_args.source_lang is not None and data_args.target_lang is not None:\n        tokenizer.src_lang = data_args.source_lang\n        tokenizer.tgt_lang = data_args.target_lang\n    \n        if isinstance(tokenizer, (MBartTokenizer, MBartTokenizerFast)):\n            if isinstance(tokenizer, MBartTokenizer):\n                model.config.decoder_start_token_id = tokenizer.lang_code_to_id[data_args.target_lang]\n            else:\n                model.config.decoder_start_token_id = tokenizer.convert_tokens_to_ids(data_args.target_lang)\n        elif isinstance(tokenizer, AlbertTokenizer):\n            model.config.decoder_start_token_id = tokenizer._convert_token_to_id_with_added_voc(tokenizer.tgt_lang)\n\n\n    prefix = data_args.source_prefix if data_args.source_prefix is not None else \"\"\n\n    for data_type, ds in raw_datasets.items():\n        assert data_args.source_key in ds.features, f\"Input files doesnt have the `{data_args.source_key}` key\"\n        if data_type != \"test\":\n            assert data_args.target_key in ds.features, f\"Input files doesnt have the `{data_args.target_key}` key\"\n        \n        ignored_columns = set(ds.column_names) - set([data_args.source_key, data_args.target_key])\n        raw_datasets[data_type] = ds.remove_columns(ignored_columns)\n\n    max_target_length = data_args.max_target_length\n    \n    def preprocess_function(examples):\n        normalization_kwargs = {\n            \"unicode_norm\": data_args.unicode_norm,\n            \"punct_replacement\": \" \" if data_args.remove_punct else None,\n            \"url_replacement\": \" \" if data_args.remove_urls else None,\n            \"emoji_replacement\": \" \" if data_args.remove_emoji else None\n        }\n        \n        \n        inputs = [normalize(ex, **normalization_kwargs) if data_args.do_normalize else ex \n                    for ex in examples[data_args.source_key]]\n        inputs = [prefix + inp for inp in inputs]\n\n        tokenizer_kwargs = {\n            \"max_length\": data_args.max_source_length, \n            \"padding\": False,\n            \"truncation\": True,\n            \"return_tensors\": \"np\"\n        }\n        \n        if is_indicbart:\n            if is_unified and tokenizer.src_lang in code2script:\n                inputs = [UnicodeIndicTransliterator.transliterate(k, code2script[tokenizer.src_lang], \"hi\")\n                            for k in inputs]\n\n            tokenizer_kwargs.update({\"add_special_tokens\": False})\n        \n        model_inputs = tokenizer(inputs, **tokenizer_kwargs)\n\n        if is_indicbart:\n            model_inputs[\"input_ids\"] = np.concatenate(\n                (   \n                    model_inputs[\"input_ids\"], \n                    np.array([[eos_id, tokenizer._convert_token_to_id_with_added_voc(tokenizer.src_lang)]]),\n                ),\n                axis=1\n            )\n            model_inputs.pop(\"token_type_ids\")\n            model_inputs[\"attention_mask\"] = np.ones_like(model_inputs[\"input_ids\"])\n\n        if data_args.target_key in examples:\n            targets = [normalize(ex, **normalization_kwargs) if data_args.do_normalize else ex\n                        for ex in examples[data_args.target_key]]\n\n            tokenizer_kwargs.update({\"max_length\": max_target_length})\n            \n            if is_unified and tokenizer.tgt_lang in code2script :\n                targets = [UnicodeIndicTransliterator.transliterate(k, code2script[tokenizer.tgt_lang], \"hi\")\n                            for k in targets]\n\n            with tokenizer.as_target_tokenizer():\n                labels = tokenizer(targets, **tokenizer_kwargs)\n\n            if is_indicbart:\n                labels[\"input_ids\"] = np.concatenate(\n                    (\n                        labels[\"input_ids\"], \n                        np.array([[eos_id, tokenizer._convert_token_to_id_with_added_voc(tokenizer.tgt_lang)]])\n                    ),\n                    # LID will get wrapped around through modeling_mbart\n                    axis=1\n                )\n                \n            model_inputs[\"labels\"] = labels[\"input_ids\"]\n\n        return model_inputs\n\n    if training_args.do_train:\n        if \"train\" not in raw_datasets:\n            raise ValueError(\"--do_train requires a train dataset\")\n        train_dataset = raw_datasets[\"train\"]\n        if data_args.max_train_samples is not None:\n            train_dataset = train_dataset.select(range(data_args.max_train_samples))\n        \n        with training_args.main_process_first(desc=\"train dataset map pre-processing\"):\n            train_dataset = train_dataset.map(\n                preprocess_function,\n                batched=True,\n                batch_size=1 if is_indicbart else training_args.train_batch_size,\n                num_proc=data_args.preprocessing_num_workers,\n                remove_columns=train_dataset.column_names,\n                load_from_cache_file=not data_args.overwrite_cache,\n                desc=\"Running tokenizer on train dataset\",\n            )\n\n    if training_args.do_eval:\n        max_target_length = data_args.val_max_target_length\n        if \"validation\" not in raw_datasets:\n            raise ValueError(\"--do_eval requires a validation dataset\")\n        eval_dataset = raw_datasets[\"validation\"]\n        if data_args.max_eval_samples is not None:\n            eval_dataset = eval_dataset.select(range(data_args.max_eval_samples))\n        with training_args.main_process_first(desc=\"validation dataset map pre-processing\"):\n            eval_dataset = eval_dataset.map(\n                preprocess_function,\n                batched=True,\n                batch_size=1 if is_indicbart else training_args.train_batch_size,\n                num_proc=data_args.preprocessing_num_workers,\n                remove_columns=eval_dataset.column_names,\n                load_from_cache_file=not data_args.overwrite_cache,\n                desc=\"Running tokenizer on validation dataset\",\n            )\n\n    if training_args.do_predict:\n        max_target_length = data_args.val_max_target_length\n        if \"test\" not in raw_datasets:\n            raise ValueError(\"--do_predict requires a test dataset\")\n        predict_dataset = raw_datasets[\"test\"]\n        if data_args.max_predict_samples is not None:\n            predict_dataset = predict_dataset.select(range(data_args.max_predict_samples))\n        with training_args.main_process_first(desc=\"prediction dataset map pre-processing\"):\n            predict_dataset = predict_dataset.map(\n                preprocess_function,\n                batched=True,\n                batch_size=1 if is_indicbart else training_args.train_batch_size,\n                num_proc=data_args.preprocessing_num_workers,\n                remove_columns=predict_dataset.column_names,\n                load_from_cache_file=not data_args.overwrite_cache,\n                desc=\"Running tokenizer on prediction dataset\",\n            )\n\n    data_collator = DataCollatorForSeq2Seq(\n        tokenizer,\n        model=model,\n        padding=True,\n        label_pad_token_id=tokenizer.pad_token_id,\n        pad_to_multiple_of=8 if training_args.fp16 else None,\n    )\n\n    def extract_rouge_mid_statistics(dct):\n        new_dict = {}\n        for k1, v1 in dct.items():\n            mid = v1.mid\n            new_dict[k1] = {stat: round(getattr(mid, stat), 4) for stat in [\"precision\", \"recall\", \"fmeasure\"]}\n        return new_dict\n\n    def add_newline_to_end_of_each_sentence(x):\n        return \"\\n\".join(nltk.sent_tokenize(x))\n\n    def process_decoded_lines(lines):\n        if is_unified and tokenizer.tgt_lang in code2script:\n            lines = [UnicodeIndicTransliterator.transliterate(k, \"hi\", code2script[tokenizer.tgt_lang])\n                            for k in lines]\n\n        return lines\n\n\n    def calculate_rouge(\n        pred_lns,\n        tgt_lns,\n        use_stemmer=True,\n        rouge_keys=[\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"],\n        return_precision_and_recall=False,\n        bootstrap_aggregation=True,\n        newline_sep=True,\n        rouge_lang=data_args.rouge_lang,\n    ):\n        \n        logger.info(\"Rouge lang: \" + str(rouge_lang))\n        scorer = rouge_scorer.RougeScorer(\n            rouge_keys, lang=rouge_lang,\n            use_stemmer=use_stemmer\n        )\n        aggregator = scoring.BootstrapAggregator()\n        for pred, tgt in zip(tgt_lns, pred_lns):\n            # rougeLsum expects \"\\n\" separated sentences within a summary\n            if newline_sep:\n                pred = add_newline_to_end_of_each_sentence(pred)\n                tgt = add_newline_to_end_of_each_sentence(tgt)\n            scores = scorer.score(pred, tgt)\n            aggregator.add_scores(scores)\n\n        if bootstrap_aggregation:\n            result = aggregator.aggregate()\n            if return_precision_and_recall:\n                return extract_rouge_mid_statistics(result)  # here we return dict\n            else:\n                return {k: round(v.mid.fmeasure * 100, 4) for k, v in result.items()}\n\n        else:\n            return aggregator._scores  # here we return defaultdict(list)\n\n  \n    def calculate_bleu(pred_lns, tgt_lns, **kwargs):\n        return {\n            \"sacrebleu\": round(\n                corpus_bleu(\n                    [k.strip() for k in pred_lns], \n                    [[k.strip() for k in tgt_lns]], \n                    **kwargs).score, \n                4)\n        }\n\n    \n    metric_fn = calculate_rouge if data_args.evaluation_metric == \"rouge\" else calculate_bleu\n\n    def compute_metrics2(eval_preds):\n        preds, labels = eval_preds\n        if isinstance(preds, tuple):\n            preds = preds[0]\n\n        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    \n        decoded_preds = process_decoded_lines(tokenizer.batch_decode(preds, skip_special_tokens=True))\n        decoded_labels = process_decoded_lines(tokenizer.batch_decode(labels, skip_special_tokens=True))\n\n        result = metric_fn(decoded_preds, decoded_labels)\n        prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n        result[\"gen_len\"] = np.mean(prediction_lens)\n        result = {k: round(v, 4) for k, v in result.items()}\n        \n        return result\n\n    def compute_metrics(eval_preds):\n        preds, labels = eval_preds\n        if isinstance(preds, tuple):\n            preds = preds[0]\n\n        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    \n        decoded_preds = process_decoded_lines(tokenizer.batch_decode(preds, skip_special_tokens=True))\n        decoded_labels = process_decoded_lines(tokenizer.batch_decode(labels, skip_special_tokens=True))\n        \n        decoded_preds2 = [x.replace(\"$\",\"\") for x in decoded_preds]\n        decoded_labels2 = [x.replace(\"$\",\"\") for x in decoded_labels]\n        \n        sum_distance = distance(decoded_preds, decoded_labels)\n        \n        sum_distance2 = distance(decoded_preds2, decoded_labels2)\n        \n        result = {}\n        result[\"lev\"] = sum_distance\n        result[\"lev2\"] = sum_distance2\n#         result[\"sacrebleu\"] = sum_distance\n        prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n        result[\"gen_len\"] = np.mean(prediction_lens)\n        result = {k: round(v, 4) for k, v in result.items()}\n        \n        return result\n\n    # Initialize our Trainer\n    trainer = Seq2SeqTrainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset if training_args.do_train else None,\n        eval_dataset=eval_dataset if training_args.do_eval else None,\n        tokenizer=tokenizer,\n        data_collator=data_collator,\n        compute_metrics=compute_metrics if training_args.predict_with_generate else None,\n    )\n\n    # Training\n    if training_args.do_train:\n        checkpoint = None\n        if training_args.resume_from_checkpoint is not None:\n            checkpoint = get_last_checkpoint(training_args.output_dir)\n        \n        train_result = trainer.train(resume_from_checkpoint=checkpoint)\n        trainer.save_model() \n\n        metrics = train_result.metrics\n        max_train_samples = (\n            data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)\n        )\n        metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n\n        trainer.log_metrics(\"train\", metrics)\n        trainer.save_metrics(\"train\", metrics)\n        trainer.save_state()\n\n    # Evaluation\n    results = {}\n    max_length = (\n        training_args.generation_max_length\n        if training_args.generation_max_length is not None\n        else data_args.val_max_target_length\n    )\n    num_beams = data_args.num_beams if data_args.num_beams is not None else training_args.generation_num_beams\n#     if training_args.do_eval:\n#         logger.info(\"*** Evaluate ***\")\n\n#         metrics = trainer.evaluate(max_length=max_length, num_beams=num_beams, metric_key_prefix=\"eval\")\n#         max_eval_samples = data_args.max_eval_samples if data_args.max_eval_samples is not None else len(eval_dataset)\n#         metrics[\"eval_samples\"] = min(max_eval_samples, len(eval_dataset))\n\n#         results.update(metrics)\n#         trainer.log_metrics(\"eval\", metrics)\n#         trainer.save_metrics(\"eval\", metrics)\n#     if training_args.do_predict:\n#         logger.info(\"*** Predict ***\")\n\n#         predict_results = trainer.predict(\n#             train_dataset, metric_key_prefix=\"predict\", max_length=max_length, num_beams=num_beams\n#         )\n#         print(\"predictions done\")\n#         metrics = predict_results.metrics\n#         max_predict_samples = (\n#             data_args.max_predict_samples if data_args.max_predict_samples is not None else len(predict_dataset)\n#         )\n#         metrics[\"predict_samples\"] = min(max_predict_samples, len(predict_dataset))\n\n#         results.update(metrics)\n#         trainer.log_metrics(\"predict\", metrics)\n#         trainer.save_metrics(\"predict\", metrics)\n        \n#         print(\"will write\")\n#         if trainer.is_world_process_zero():\n#             if training_args.predict_with_generate:\n#                 predictions = process_decoded_lines(\n#                     tokenizer.batch_decode(\n#                         predict_results.predictions, skip_special_tokens=True, clean_up_tokenization_spaces=True\n#                     )\n#                 )\n#                 predictions = [pred.strip() for pred in predictions]\n#                 output_prediction_file = os.path.join(training_args.output_dir, \"train_predictions.txt\")\n#                 with open(output_prediction_file, \"w\", encoding=\"utf-8\") as writer:\n#                     writer.write(\"\\n\".join(predictions))\n#                 print(\"write done\")\n    \n    \n    if training_args.do_predict:\n        logger.info(\"*** Predict ***\")\n\n        predict_results = trainer.predict(\n            predict_dataset, metric_key_prefix=\"predict\", max_length=max_length, num_beams=num_beams\n        )\n        print(\"predictions done\")\n        metrics = predict_results.metrics\n        max_predict_samples = (\n            data_args.max_predict_samples if data_args.max_predict_samples is not None else len(predict_dataset)\n        )\n        metrics[\"predict_samples\"] = min(max_predict_samples, len(predict_dataset))\n\n        results.update(metrics)\n        trainer.log_metrics(\"predict\", metrics)\n        trainer.save_metrics(\"predict\", metrics)\n        \n        print(\"will write\")\n        if trainer.is_world_process_zero():\n            if training_args.predict_with_generate:\n                predictions = process_decoded_lines(\n                    tokenizer.batch_decode(\n                        predict_results.predictions, skip_special_tokens=True, clean_up_tokenization_spaces=True\n                    )\n                )\n                predictions = [pred.strip() for pred in predictions]\n                output_prediction_file = os.path.join(training_args.output_dir, \"generated_predictions.txt\")\n                with open(output_prediction_file, \"w\", encoding=\"utf-8\") as writer:\n                    writer.write(\"\\n\".join(predictions))\n                print(\"write done\")\n    all_results_path = os.path.join(training_args.output_dir, \"all_results.json\")\n    with open(all_results_path, 'w') as f:\n        json.dump(results, f, indent=4, ensure_ascii=False)\n\n    \n\n","metadata":{"execution":{"iopub.status.busy":"2023-03-10T02:57:29.822770Z","iopub.execute_input":"2023-03-10T02:57:29.823299Z","iopub.status.idle":"2023-03-10T02:57:41.734485Z","shell.execute_reply.started":"2023-03-10T02:57:29.823248Z","shell.execute_reply":"2023-03-10T02:57:41.733471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# python ./run_seq2seq.py \\\n#     --model_name_or_path \"csebuetnlp/banglat5\" \\\n#     --dataset_dir \"sample_inputs/\" \\\n#     --output_dir \"outputs/\" \\\n#     --learning_rate=5e-4 \\\n#     --warmup_steps 5000 \\\n#     --label_smoothing_factor 0.1 \\\n#     --gradient_accumulation_steps 4 \\\n#     --weight_decay 0.1 \\\n#     --lr_scheduler_type \"linear\"  \\\n#     --per_device_train_batch_size=8 \\\n#     --per_device_eval_batch_size=8 \\\n#     --max_source_length 256 \\\n#     --max_target_length 256 \\\n#     --logging_strategy \"epoch\" \\\n#     --save_strategy \"epoch\" \\\n#     --evaluation_strategy \"epoch\" \\\n#     --source_key bn --target_key en \\\n#     --greater_is_better true --load_best_model_at_end \\\n#     --metric_for_best_model sacrebleu --evaluation_metric sacrebleu \\\n#     --num_train_epochs 20 \\\n#     --do_train --do_eval --do_predict \\\n#     --predict_with_generate","metadata":{"execution":{"iopub.status.busy":"2023-03-10T02:57:41.736146Z","iopub.execute_input":"2023-03-10T02:57:41.737021Z","iopub.status.idle":"2023-03-10T02:57:41.741888Z","shell.execute_reply.started":"2023-03-10T02:57:41.736983Z","shell.execute_reply":"2023-03-10T02:57:41.740931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !rm -rf /kaggle/working/banglat5","metadata":{"execution":{"iopub.status.busy":"2023-03-10T02:57:41.743422Z","iopub.execute_input":"2023-03-10T02:57:41.743995Z","iopub.status.idle":"2023-03-10T02:57:41.756275Z","shell.execute_reply.started":"2023-03-10T02:57:41.743958Z","shell.execute_reply":"2023-03-10T02:57:41.755375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_args = ModelArguments(\"csebuetnlp/banglat5_small\"\n                           )\n# model_args = ModelArguments(\"/kaggle/working/banglat5\")\ndata_args = DataTrainingArguments(dataset_dir = \"/kaggle/working/json_data\", \n                                  evaluation_metric= \"sacrebleu\",\n                                      max_source_length= 256 ,\n                                    max_target_length= 256 ,\n                                 )\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"banglat5/\",\n    learning_rate=5e-4,\n    warmup_steps = 5000,\n    label_smoothing_factor= 0.1 ,\n    weight_decay =0.1 ,\n    lr_scheduler_type =\"linear\" ,\n    per_device_train_batch_size=32 ,\n    per_device_eval_batch_size=32,\n    logging_strategy= \"epoch\", \n    save_strategy =\"epoch\" ,\n    evaluation_strategy =\"epoch\",\n#     greater_is_better= False,\n#     load_best_model_at_end = True,\n#     metric_for_best_model = \"lev\" ,\n    report_to = [],\n    save_total_limit=2,\n    gradient_accumulation_steps=4,\n#     gradient_accumulation_step= 4 ,\n    resume_from_checkpoint = True,\n    num_train_epochs =  120,\n    do_train = True,\n    do_predict = True,\n    do_eval = True,\n    predict_with_generate = True,\n)\n","metadata":{"execution":{"iopub.status.busy":"2023-03-10T02:57:41.757605Z","iopub.execute_input":"2023-03-10T02:57:41.758073Z","iopub.status.idle":"2023-03-10T02:57:41.773747Z","shell.execute_reply.started":"2023-03-10T02:57:41.758037Z","shell.execute_reply":"2023-03-10T02:57:41.772893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data_args.do_normalize = True","metadata":{"execution":{"iopub.status.busy":"2023-03-10T02:57:41.775476Z","iopub.execute_input":"2023-03-10T02:57:41.775873Z","iopub.status.idle":"2023-03-10T02:57:41.781732Z","shell.execute_reply.started":"2023-03-10T02:57:41.775840Z","shell.execute_reply":"2023-03-10T02:57:41.780549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !rm -rf /kaggle/working/banglat5","metadata":{"execution":{"iopub.status.busy":"2023-03-10T02:57:41.785091Z","iopub.execute_input":"2023-03-10T02:57:41.785541Z","iopub.status.idle":"2023-03-10T02:57:41.791713Z","shell.execute_reply.started":"2023-03-10T02:57:41.785516Z","shell.execute_reply":"2023-03-10T02:57:41.790810Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"main(model_args, data_args, training_args)","metadata":{"execution":{"iopub.status.busy":"2023-03-10T02:57:41.792948Z","iopub.execute_input":"2023-03-10T02:57:41.793405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}